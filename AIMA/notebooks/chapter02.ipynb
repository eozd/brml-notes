{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents and Environments\n",
    "\n",
    "#### Agent\n",
    "An agent is anything that perceives its environment through sensors and acts upon the environment through actuators.\n",
    "\n",
    "#### Percept\n",
    "Agent's perceptual inputs at any given instant.\n",
    "\n",
    "An agent's percept sequence is the complete history of everything the agent has ever perceived.\n",
    "  * Infinite sequence.\n",
    "  \n",
    "Abstractly, an agent is a function that maps the current percept sequence to an action.\n",
    "  * The set of possible percept sequences is infinite; hence, writing out a exhaustive table for the agent function is infeasible.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationality\n",
    "An agent behaves rationally if the sequence of states its actions cause have optimal **performance measure**.\n",
    "\n",
    "---\n",
    "\n",
    "As a general rule, performance measures should be designed according to what one actually wants in the environment rather than according to how one thinks the agent should behave.\n",
    "\n",
    "Rationality depends on\n",
    "\n",
    "1. performance measure\n",
    "2. prior knowledge of the environment\n",
    "3. possible actions\n",
    "4. percept sequence\n",
    "\n",
    "_For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and the prior knowledge of the environment._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rationality vs Perfection\n",
    "Rationality maximizes expected performance while perfection maximizes actual performance.\n",
    "\n",
    "A perfect agent is impossible in practice since it must be omniscient. An omniscient agent knows the **actual** outcomes of its actions, not expected outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autonomy\n",
    "To the extent that an agent relies on the prior knowledge of its designer rather than its own percepts, we say that the agent lacks autonomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Environment (PEAS)\n",
    "Task environments are essentially problems to which the agents are the solutions.\n",
    "\n",
    "1. Performance\n",
    "2. Environment\n",
    "3. Actuators\n",
    "4. Sensors\n",
    "\n",
    "In designing an agent, we first need to specify the task environment.\n",
    "\n",
    "#### Fully Observable vs. Partially Observable\n",
    "* **Fully observable:** The sensors specify the environment completely\n",
    "* **Partially observable:** The sensors specify only some parts of the environment.\n",
    "\n",
    "#### Single Agent vs. Multi Agent\n",
    "An agent $A$ considers on object $B$ as an agent if $B$'s behavior is best decribed by maximizing a performance measure whose value depends on $A$'s behavior. (so there should be some kind of direct/indirect dependence between agents)\n",
    "\n",
    "#### Deterministic vs. Stochastic\n",
    "If the next state is completely determined by the current state and the agent's action, the environment is deterministic. On the other hand, if there is randomness involved in determining the next state, the environment is stochastic.\n",
    "\n",
    "---\n",
    "\n",
    "Consider a partially observable environment. There is uncertainty related to the unobserved parts of the environment.\n",
    "\n",
    "If an environment is nondeterministic or partially observable, then it is **uncertain**.\n",
    "\n",
    "#### Episodic vs. Sequential\n",
    "In episodic environments, the timeline is divided into episodes and **different episodes are independent of each other.** However, in sequential environments, the current state may affect **all future states.**\n",
    "\n",
    "An environment may be episodic where each episode is sequential in itself.\n",
    "\n",
    "#### Static vs. Dynamic\n",
    "If the environment changes while the agent is making a decision, it is dynamic; otherwise, it is static.\n",
    "  * Dynamic environments are continuously asking the agent what it wants to do; if it hasn't decided yet, **that counts as doing nothing.**\n",
    "  \n",
    "---\n",
    "\n",
    "An environment is semidynamic if the environment itself doesn't change with time but the agent's performance measure does.\n",
    "\n",
    "#### Discrete vs. Continuous\n",
    "Discrete/continuous space, discrete/continuous time, etc.\n",
    "\n",
    "#### Known vs. Unknown\n",
    "Not about the environment and observations about the environment, but rather the rules of the environment. In a known environment, the outcomes of all actions are known (the agent knows the expected outcome of its actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of Agents\n",
    "\n",
    "#### Agent Program\n",
    "Takes the current percept as input and returns the current action.\n",
    "\n",
    "If need to rely on the percept sequence, the program has to store the past percepts.\n",
    "\n",
    "#### Simple Reflex Agent\n",
    "Take action according to the current percept, ignoring the rest of the percept history.\n",
    "\n",
    "#### Model-based Reflex Agent\n",
    "Keep a model of the world as an internal state that describes the unobserved parts of the environment according to the percept history.\n",
    "Then, when deciding, use the information coming from the model to answer \"how the world works\" questions.\n",
    "\n",
    "These agents need to know two types of information about the world:\n",
    "1. How the world itself evolves?\n",
    "2. How my actions affect the world?\n",
    "\n",
    "#### Goal-based Agent\n",
    "The agent has a goal state. In these cases, searching and planning are used.\n",
    "\n",
    "Goal-based agents are generally more flexible than reflex agents, because you don't need to rewrite whole tables when the environment changes.\n",
    "\n",
    "#### Utility-based Agent\n",
    "Goal-based agents classify the states into binary classes: goal and no-goal. Utility-based agents associate a utility function with each state, and try to maximize this utility function.\n",
    "\n",
    "If the utility function is chosen such that the utility function and external performance measurement are in line, then an agent that maximizes its utility is a rational one.\n",
    "\n",
    "A utility based agent maximizes the expected utility of the action outcomes given the probabilities and utilities of each outcome.\n",
    "\n",
    "#### Learning Agent\n",
    "* **Performance element:** Select the best external action.\n",
    "* **Critic:** Give feedback about the agent's external actions.\n",
    "* **Learning element:** Make improvements on the performance element according to the feedback from the critic.\n",
    "* **Problem generator:** Suggest possibly suboptimal but new actions so that the agent explores new possibilities which ultimately may lead to better outcomes.\n",
    "\n",
    "All previous agents consisted only of the performance element. Hence, all the previous designs can be turned to a learning agent by incorporating critic and learning element parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Representation\n",
    "\n",
    "#### Atomic\n",
    "Each individual state is indivisible and has no internal configuration.\n",
    "1. search and game-playing algorithms\n",
    "2. HMMs\n",
    "3. Markov Decision Processes, etc.\n",
    "\n",
    "#### Factored\n",
    "Each state is a fixed set of variables and attributes. In this representation, different states may have things in common (certain variables having the same value).\n",
    "1. constraint satisfaction\n",
    "2. Bayes nets\n",
    "3. planning, etc.\n",
    "\n",
    "#### Structured\n",
    "Variables in the states are related as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state(m, n):\n",
    "    return np.zeros((m, n), dtype=int)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        if np.sum(state) == 0:\n",
    "            return 'noop'\n",
    "        elif state[self.x, self.y] == 1:\n",
    "            return 'eat'\n",
    "        elif self.y == 0:\n",
    "            return 'right'\n",
    "        else:\n",
    "            return 'left'\n",
    "        \n",
    "class Simulator:\n",
    "    def __init__(self, state, agent):\n",
    "        self.cost = 0\n",
    "        self.state = state\n",
    "        self.agent = agent\n",
    "        \n",
    "    def simulate(self, t=10):\n",
    "        for i in range(t):\n",
    "            action = self.agent.take_action(self.state)\n",
    "            if action == 'eat':\n",
    "                x = self.agent.x\n",
    "                y = self.agent.y\n",
    "                self.state[x, y] = 0\n",
    "                self.cost += 1\n",
    "            elif action == 'left':\n",
    "                self.agent.y = max(self.agent.y - 1, 0)\n",
    "                self.cost += 1\n",
    "            elif action == 'right':\n",
    "                self.agent.y = min(self.agent.y + 1, self.state.shape[1])\n",
    "                self.cost += 1\n",
    "            elif action == 'noop':\n",
    "                pass\n",
    "            \n",
    "            header = 'Step {}'.format(i)\n",
    "            #print(header)\n",
    "            #print('-'*len(header))\n",
    "            #print('({}, {})'.format(self.agent.x, self.agent.y))\n",
    "            #print(state)\n",
    "        #print('Final cost: {}'.format(self.curr_cost))          \n",
    "        \n",
    "state = make_state(5, 5)\n",
    "#state[3, 4] = 1\n",
    "state[4, 4] = 1\n",
    "state[4, 2] = 1\n",
    "agent = Agent(4, 4)\n",
    "simulator = Simulator(state, agent)\n",
    "simulator.simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [[0 0]]\n",
      "Agent: (0, 0)\n",
      "Cost: 0\n",
      "State: [[0 0]]\n",
      "Agent: (0, 1)\n",
      "Cost: 0\n",
      "State: [[0 1]]\n",
      "Agent: (0, 0)\n",
      "Cost: 2\n",
      "State: [[0 1]]\n",
      "Agent: (0, 1)\n",
      "Cost: 1\n",
      "State: [[1 0]]\n",
      "Agent: (0, 0)\n",
      "Cost: 1\n",
      "State: [[1 0]]\n",
      "Agent: (0, 1)\n",
      "Cost: 2\n",
      "State: [[1 1]]\n",
      "Agent: (0, 0)\n",
      "Cost: 3\n",
      "State: [[1 1]]\n",
      "Agent: (0, 1)\n",
      "Cost: 3\n",
      "Avg. cost: 1.50\n"
     ]
    }
   ],
   "source": [
    "state = make_state(1, 2)\n",
    "total_cost = 0\n",
    "total_sim = 0\n",
    "\n",
    "for conf in product((0, 1), repeat=2):\n",
    "    for y in range(2):\n",
    "        state[0] = conf\n",
    "        agent = Agent(0, y)\n",
    "        simulator = Simulator(state, agent)\n",
    "        \n",
    "        print('State: {}'.format(state))\n",
    "        print('Agent: {}'.format((agent.x, agent.y)))\n",
    "         \n",
    "        simulator.simulate()\n",
    "        cost = simulator.cost\n",
    "        total_cost += cost\n",
    "        total_sim += 1\n",
    "    \n",
    "        print('Cost: {}'.format(cost))\n",
    "        \n",
    "print('Avg. cost: {:.2f}'.format(total_cost/total_sim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
